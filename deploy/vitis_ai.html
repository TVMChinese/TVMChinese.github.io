





<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vitis-AI Integration &mdash; tvm 0.8.dev1728+g18870dfb0 文档</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="Relay BNNS Integration" href="bnns.html" />
    <link rel="prev" title="Relay TensorRT Integration" href="tensorrt.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmchinese.github.io/declaration_zh_CN.html>Translate-Contribution</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.8.dev1728+g18870dfb0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">引导</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/index.html">为 TVM 做出贡献</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">部署和集成</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#build-the-tvm-runtime-library">构建 TVM 运行 runtime 库</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#cross-compile-the-tvm-runtime-for-other-architectures">为其它架构交叉编译TVM runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#optimize-and-tune-models-for-target-devices">针对目标设备优化和调整模型</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#deploy-optimized-model-on-target-devices">在目标设备上部署优化的模型</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="cpp_deploy.html">Deploy TVM Module using C++ API</a></li>
<li class="toctree-l3"><a class="reference internal" href="android.html">部署到安卓</a></li>
<li class="toctree-l3"><a class="reference internal" href="integrate.html">Integrate TVM into Your Project</a></li>
<li class="toctree-l3"><a class="reference internal" href="hls.html">HLS Backend Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="arm_compute_lib.html">Relay Arm<sup>®</sup> Compute Library Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorrt.html">Relay TensorRT Integration</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Vitis-AI Integration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dpu-naming-information">DPU naming information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build-instructions">Build instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started">Getting started</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="bnns.html">Relay BNNS Integration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/how_to.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../microtvm/index.html">microTVM：裸机使用TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../errors.html">使用 TVM 时遇到 Errors 怎么做</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">常见提问</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">教程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Getting Started With TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#compile-deep-learning-models">编译深度学习模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#autotvm-template-based-auto-tuning">AutoTVM : Template-based Auto Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#autoscheduler-template-free-auto-scheduling">AutoScheduler : Template-free Auto Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#microtvm">microTVM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../langref/index.html">语言参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/links.html">其它 API 参考链接</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">深入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Design and Architecture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">杂项</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling/index.html">Profiling Deep Learning Models</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of content
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="index.html">部署和集成</a> <span class="br-arrow">></span></li>
        
      <li>Vitis-AI Integration</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/deploy/vitis_ai.rst.txt" rel="nofollow"> <img src="../_static//img/source.svg" alt="viewsource"/></a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vitis-ai-integration">
<h1>Vitis-AI Integration<a class="headerlink" href="#vitis-ai-integration" title="永久链接至标题">¶</a></h1>
<p><a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis-AI</a> is Xilinx’s
development stack for hardware-accelerated AI inference on Xilinx
platforms, including both edge devices and Alveo cards. It consists of
optimized IP, tools, libraries, models, and example designs. It is
designed with high efficiency and ease of use in mind, unleashing the
full potential of AI acceleration on Xilinx FPGA and ACAP.</p>
<p>The current Vitis-AI Byoc flow inside TVM enables acceleration of Neural
Network model inference on edge and cloud. The identifiers for the
supported edge and cloud Deep Learning Processor Units (DPU’s) are
DPUCZDX8G respectively DPUCADX8G. DPUCZDX8G and DPUCADX8G are hardware
accelerators for convolutional neural networks (CNN’s) on top of the
Xilinx <a class="reference external" href="https://www.xilinx.com/products/silicon-devices/soc/zynq-ultrascale-mpsoc.html">Zynq Ultrascale+
MPSoc</a>
respectively
<a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/alveo.html">Alveo</a>
(U200/U250) platforms. For more information about the DPU identifiers
see the section on <a class="reference external" href="#dpu-naming-information">DPU naming information</a>.</p>
<p>On this page you will find information on how to
<a class="reference external" href="#build-instructions">build</a> TVM with Vitis-AI and on how to <a class="reference external" href="#getting-started">get
started</a> with an example.</p>
<div class="section" id="dpu-naming-information">
<h2>DPU naming information<a class="headerlink" href="#dpu-naming-information" title="永久链接至标题">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 6%" />
<col style="width: 24%" />
<col style="width: 19%" />
<col style="width: 17%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>DPU</p></th>
<th class="head"><p>Application</p></th>
<th class="head"><p>HW Platform</p></th>
<th class="head"><p>Quantization Method</p></th>
<th class="head"><p>Quantization Bitwidth</p></th>
<th class="head"><p>Design Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep Learning Processing Unit</p></td>
<td><p>C: CNN R: RNN</p></td>
<td><p>AD: Alveo DDR AH: Alveo HBM VD: Versal DDR with AIE &amp; PL ZD: Zynq DDR</p></td>
<td><p>X: DECENT I: Integer threshold F: Float threshold R: RNN</p></td>
<td><p>4: 4-bit 8: 8-bit 16: 16-bit M: Mixed Precision</p></td>
<td><p>G: General purpose H: High throughput L: Low latency C: Cost optimized</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="build-instructions">
<h2>Build instructions<a class="headerlink" href="#build-instructions" title="永久链接至标题">¶</a></h2>
<p>This section lists the instructions for building TVM with Vitis-AI for
both <a class="reference external" href="#cloud-dpucadx8g">cloud</a> and <a class="reference external" href="#edge-dpuczdx8g">edge</a>.</p>
<div class="section" id="cloud-dpucadx8g">
<h3>Cloud (DPUCADX8G)<a class="headerlink" href="#cloud-dpucadx8g" title="永久链接至标题">¶</a></h3>
<p>For Vitis-AI acceleration in the cloud TVM has to be built on top of the
Xilinx Alveo platform.</p>
<div class="section" id="system-requirements">
<h4>System requirements<a class="headerlink" href="#system-requirements" title="永久链接至标题">¶</a></h4>
<p>The following table lists system requirements for running docker
containers as well as Alveo cards.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Component</strong></p></th>
<th class="head"><p><strong>Requirement</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Motherboard</p></td>
<td><p>PCI Express 3.0-compliant with one dual-width x16 slot</p></td>
</tr>
<tr class="row-odd"><td><p>System Power Supply</p></td>
<td><p>225W</p></td>
</tr>
<tr class="row-even"><td><p>Operating System</p></td>
<td><p>Ubuntu 16.04, 18.04</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>CentOS 7.4, 7.5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>RHEL 7.4, 7.5</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>Intel i3/i5/i7/i9/Xeon 64-bit CPU</p></td>
</tr>
<tr class="row-even"><td><p>GPU (Optional to accelerate quantization)</p></td>
<td><p>NVIDIA GPU with a compute capability &gt; 3.0</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA Driver (Optional to accelerate quantization)</p></td>
<td><p>nvidia-410</p></td>
</tr>
<tr class="row-even"><td><p>FPGA</p></td>
<td><p>Xilinx Alveo U200 or U250</p></td>
</tr>
<tr class="row-odd"><td><p>Docker Version</p></td>
<td><p>19.03.1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="hardware-setup-and-docker-build">
<h4>Hardware setup and docker build<a class="headerlink" href="#hardware-setup-and-docker-build" title="永久链接至标题">¶</a></h4>
<ol class="arabic">
<li><p>Clone the Vitis AI repository:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recurse-submodules https://github.com/Xilinx/Vitis-AI
</pre></div>
</div>
</li>
<li><p>Install Docker, and add the user to the docker group. Link the user
to docker installation instructions from the following docker’s
website:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a></p></li>
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/docker-ce/centos/">https://docs.docker.com/install/linux/docker-ce/centos/</a></p></li>
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/linux-postinstall/">https://docs.docker.com/install/linux/linux-postinstall/</a></p></li>
</ul>
</li>
<li><p>Download the latest Vitis AI Docker with the following command. This container runs on CPU.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker pull xilinx/vitis-ai:latest
</pre></div>
</div>
<p>To accelerate the quantization, you can optionally use the Vitis-AI GPU docker image. Use the below commands to build the Vitis-AI GPU docker container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> Vitis-AI/docker
./docker_build_gpu.sh
</pre></div>
</div>
</li>
<li><p>Set up Vitis AI to target Alveo cards. To target Alveo cards with
Vitis AI for machine learning workloads, you must install the
following software components:</p>
<ul class="simple">
<li><p>Xilinx Runtime (XRT)</p></li>
<li><p>Alveo Deployment Shells (DSAs)</p></li>
<li><p>Xilinx Resource Manager (XRM) (xbutler)</p></li>
<li><p>Xilinx Overlaybins (Accelerators to Dynamically Load - binary
programming files)</p></li>
</ul>
<p>While it is possible to install all of these software components
individually, a script has been provided to automatically install
them at once. To do so:</p>
<ul>
<li><p>Run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> Vitis-AI/alveo/packages
sudo su
./install.sh
</pre></div>
</div>
</li>
<li><p>Power cycle the system.</p></li>
</ul>
</li>
<li><p>Clone tvm repo and pyxir repo</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/apache/tvm.git
git clone --recursive https://github.com/Xilinx/pyxir.git
</pre></div>
</div>
</li>
<li><p>Build and start the tvm runtime Vitis-AI Docker Container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tvm/docker/build.sh demo_vitis_ai bash
./tvm/docker/bash.sh tvm.demo_vitis_ai

<span class="c1">#Setup inside container</span>
<span class="nb">source</span> /opt/xilinx/xrt/setup.sh
. <span class="nv">$VAI_ROOT</span>/conda/etc/profile.d/conda.sh
conda activate vitis-ai-tensorflow
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> pyxir
python3 setup.py install --use_vai_rt_dpucadx8g --user
</pre></div>
</div>
</li>
<li><p>Build TVM inside the container with Vitis-AI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_LLVM ON<span class="se">\)</span> &gt;&gt; config.cmake
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm/python
pip3 install -e . --user
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="edge-dpuczdx8g">
<h3>Edge (DPUCZDX8G)<a class="headerlink" href="#edge-dpuczdx8g" title="永久链接至标题">¶</a></h3>
<p>For edge deployment we make use of two systems referred to as host and
edge. The <a class="reference external" href="#host-requirements">host</a> system is responsible for
quantization and compilation of the neural network model in a first
offline step. Afterwards, the model will de deployed on the
<a class="reference external" href="#edge-requirements">edge</a> system.</p>
<div class="section" id="host-requirements">
<h4>Host requirements<a class="headerlink" href="#host-requirements" title="永久链接至标题">¶</a></h4>
<p>The following table lists system requirements for running the TVM -
Vitis-AI docker container.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 54%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Component</strong></p></th>
<th class="head"><p><strong>Requirement</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Operating System</p></td>
<td><p>Ubuntu 16.04, 18.04</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>CentOS 7.4, 7.5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>RHEL 7.4, 7.5</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>Intel i3/i5/i7/i9/Xeon 64-bit CPU</p></td>
</tr>
<tr class="row-even"><td><p>GPU (Optional to accelerate quantization)</p></td>
<td><p>NVIDIA GPU with a compute capability &gt; 3.0</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA Driver (Optional to accelerate quantization)</p></td>
<td><p>nvidia-410</p></td>
</tr>
<tr class="row-even"><td><p>FPGA</p></td>
<td><p>Not necessary on host</p></td>
</tr>
<tr class="row-odd"><td><p>Docker Version</p></td>
<td><p>19.03.1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="host-setup-and-docker-build">
<h4>Host setup and docker build<a class="headerlink" href="#host-setup-and-docker-build" title="永久链接至标题">¶</a></h4>
<ol class="arabic">
<li><p>Clone tvm repo</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/apache/tvm.git
</pre></div>
</div>
</li>
<li><p>Build and start the tvm runtime Vitis-AI Docker Container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm
./tvm/docker/build.sh demo_vitis_ai bash
./tvm/docker/bash.sh tvm.demo_vitis_ai

<span class="c1">#Setup inside container</span>
. <span class="nv">$VAI_ROOT</span>/conda/etc/profile.d/conda.sh
conda activate vitis-ai-tensorflow
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/Xilinx/pyxir.git
<span class="nb">cd</span> pyxir
python3 setup.py install --user
</pre></div>
</div>
</li>
<li><p>Build TVM inside the container with Vitis-AI.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_LLVM ON<span class="se">\)</span> &gt;&gt; config.cmake
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm/python
pip3 install -e . --user
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="edge-requirements">
<h4>Edge requirements<a class="headerlink" href="#edge-requirements" title="永久链接至标题">¶</a></h4>
<p>The DPUCZDX8G can be deployed on the <a class="reference external" href="https://www.xilinx.com/products/silicon-devices/soc/zynq-ultrascale-mpsoc.html">Zynq Ultrascale+
MPSoc</a>
platform. The following development boards can be used out-of-the-box:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Target board</strong></p></th>
<th class="head"><p><strong>TVM identifier</strong></p></th>
<th class="head"><p><strong>Info</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Ultra96</p></td>
<td><p>DPUCZDX8G-ultra96</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/1-vad4rl.html">https://www.xilinx.com/products/boards-and-kits/1-vad4rl.html</a></p></td>
</tr>
<tr class="row-odd"><td><p>ZCU104</p></td>
<td><p>DPUCZDX8G-zcu104</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/zcu104.html">https://www.xilinx.com/products/boards-and-kits/zcu104.html</a></p></td>
</tr>
<tr class="row-even"><td><p>ZCU102</p></td>
<td><p>DPUCZDX8G-zcu102</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html</a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="edge-hardware-setup">
<h4>Edge hardware setup<a class="headerlink" href="#edge-hardware-setup" title="永久链接至标题">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>This section provides instructions for setting up with the <a class="reference external" href="http://www.pynq.io/">Pynq</a> platform but
Petalinux based flows are also supported.</p>
</div>
<ol class="arabic">
<li><p>Download the Pynq v2.6 image for your target (use Z1 or Z2 for
Ultra96 target depending on board version) Link to image:
<a class="reference external" href="https://github.com/Xilinx/PYNQ/releases/tag/v2.6.0">https://github.com/Xilinx/PYNQ/releases/tag/v2.6.0</a></p></li>
<li><p>Follow Pynq instructions for setting up the board: <a class="reference external" href="https://pynq.readthedocs.io/en/latest/getting_started.html">pynq
setup</a></p></li>
<li><p>After connecting to the board, make sure to run as root. <strong>Execute</strong>
<code class="docutils literal notranslate"><span class="pre">su</span></code></p></li>
<li><p>Set up DPU on Pynq:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --branch v1.2.0 --recursive --shallow-submodules https://github.com/Xilinx/DPU-PYNQ.git
<span class="nb">cd</span> DPU-PYNQ/upgrade
make
pip3 install pynq-dpu<span class="o">==</span><span class="m">1</span>.2.0
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Run the following command to download the DPU bitstream:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 -c <span class="s1">&#39;from pynq_dpu import DpuOverlay ; overlay = DpuOverlay(&quot;dpu.bit&quot;)&#39;</span>
</pre></div>
</div>
</li>
<li><p>Check whether the DPU kernel is alive:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dexplorer -w
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="edge-tvm-setup">
<h4>Edge TVM setup<a class="headerlink" href="#edge-tvm-setup" title="永久链接至标题">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>When working on Petalinux instead of Pynq, the following steps might take more manual work (e.g building
hdf5 from source). Also, TVM has a scipy dependency which you then might have to build from source or
circumvent. We don’t depend on scipy in our flow.</p>
</div>
<p>Building TVM depends on the Xilinx
<a class="reference external" href="https://github.com/Xilinx/pyxir">PyXIR</a> package. PyXIR acts as an
interface between TVM and Vitis-AI tools.</p>
<ol class="arabic">
<li><p>First install the PyXIR h5py and pydot dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apt-get install libhdf5-dev
pip3 install <span class="nv">pydot</span><span class="o">==</span><span class="m">1</span>.4.1 <span class="nv">h5py</span><span class="o">==</span><span class="m">2</span>.8.0
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/Xilinx/pyxir.git
<span class="nb">cd</span> pyxir
sudo python3 setup.py install --use_vai_rt_dpuczdx8g
</pre></div>
</div>
</li>
<li><p>Build TVM with Vitis-AI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/apache/tvm
<span class="nb">cd</span> tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_LLVM OFF<span class="se">\)</span> &gt;&gt; config.cmake
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make tvm_runtime -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> tvm/python
pip3 install -e .
</pre></div>
</div>
</li>
<li><p>Check whether the setup was successful in the Python shell:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 -c <span class="s1">&#39;import pyxir; import tvm&#39;</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="永久链接至标题">¶</a></h2>
<p>This section shows how to use TVM with Vitis-AI. For this it’s important
to understand that neural network models are quantized for Vitis-AI
execution in fixed point arithmetic. The approach we take here is to
quantize on-the-fly using the first N inputs as explained in the next
section.</p>
<div class="section" id="on-the-fly-quantization">
<h3>On-the-fly quantization<a class="headerlink" href="#on-the-fly-quantization" title="永久链接至标题">¶</a></h3>
<p>Usually, to be able to accelerate inference of Neural Network models
with Vitis-AI DPU accelerators, those models need to quantized upfront.
In TVM - Vitis-AI flow, we make use of on-the-fly quantization to remove
this additional preprocessing step. In this flow, one doesn’t need to
quantize his/her model upfront but can make use of the typical inference
execution calls (module.run) to quantize the model on-the-fly using the
first N inputs that are provided (see more information below). This will
set up and calibrate the Vitis-AI DPU and from that point onwards
inference will be accelerated for all next inputs. Note that the edge
flow deviates slightly from the explained flow in that inference won’t
be accelerated after the first N inputs but the model will have been
quantized and compiled and can be moved to the edge device for
deployment. Please check out the <a class="reference external" href="#Edge%20usage">edge</a> usage
instructions below for more information.</p>
</div>
<div class="section" id="config-settings">
<h3>Config/Settings<a class="headerlink" href="#config-settings" title="永久链接至标题">¶</a></h3>
<p>A couple of environment variables can be used to customize the Vitis-AI
Byoc flow.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Environment Variable</strong></p></th>
<th class="head"><p><strong>Default if unset</strong></p></th>
<th class="head"><p><strong>Explanation</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PX_QUANT_SIZE</p></td>
<td><p>128</p></td>
<td><p>The number of inputs that will be used for quantization (necessary for Vitis-AI acceleration)</p></td>
</tr>
<tr class="row-odd"><td><p>PX_BUILD_DIR</p></td>
<td><p>Use the on-the-fly quantization flow</p></td>
<td><p>Loads the quantization and compilation information from the provided build directory and immediately starts Vitis-AI hardware acceleration. This configuration can be used if the model has been executed before using on-the-fly quantization during which the quantization and comilation information was cached in a build directory.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="cloud-usage">
<h3>Cloud usage<a class="headerlink" href="#cloud-usage" title="永久链接至标题">¶</a></h3>
<p>This section shows how to accelerate a convolutional neural network
model in TVM with Vitis-AI on the cloud.</p>
<p>To be able to target the Vitis-AI cloud DPUCADX8G we first have
to import the DPU target in PyXIR. This PyXIR package is the interface being
used by TVM to integrate with the Vitis-AI stack. Additionaly, import
the typical TVM and Relay modules and the Vitis-AI contrib module inside
TVM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyxir</span>
<span class="kn">import</span> <span class="nn">pyxir.contrib.target.DPUCADX8G</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.relay</span> <span class="kn">as</span> <span class="nn">relay</span>
<span class="kn">from</span> <span class="nn">tvm.contrib.target</span> <span class="kn">import</span> <span class="n">vitis_ai</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">graph_executor</span>
<span class="kn">from</span> <span class="nn">tvm.relay.build_module</span> <span class="kn">import</span> <span class="n">bind_params_by_name</span>
<span class="kn">from</span> <span class="nn">tvm.relay.op.contrib.vitis_ai</span> <span class="kn">import</span> <span class="n">partition_for_vitis_ai</span>
</pre></div>
</div>
<p>After importing a convolutional neural network model using the usual
Relay API’s, annotate the Relay expression for the given Vitis-AI DPU
target and partition the graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dpu</span> <span class="o">=</span> <span class="s1">&#39;DPUCADX8G&#39;</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">partition_for_vitis_ai</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">dpu</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can build the TVM runtime library for executing the model. The
TVM target is ‘llvm’ as the operations that can’t be handled by the DPU
are executed on the CPU. The Vitis-AI DPU is DPUCADX8G as we are
targeting the cloud DPU and this DPU identifier is passed as a config to the TVM
build call.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="s1">&#39;llvm&#39;</span>

<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;dpu&#39;</span><span class="p">:</span> <span class="n">dpu</span><span class="p">}}):</span>
   <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>As one more step before we can accelerate a model with Vitis-AI in TVM
we have to quantize and compile the model for execution on the DPU. We
make use of on-the-fly quantization for this. Using this method one
doesn’t need to quantize their model upfront and can make use of the
typical inference execution calls (module.run) to calibrate the model
on-the-fly using the first N inputs that are provided. After the first N
iterations, computations will be accelerated on the DPU. So now we will
feed N inputs to the TVM runtime module. Note that these first N inputs
will take a substantial amount of time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>

<span class="c1"># First N (default = 128) inputs are used for quantization calibration and will</span>
<span class="c1"># be executed on the CPU</span>
<span class="c1"># This config can be changed by setting the &#39;PX_QUANT_SIZE&#39; (e.g. export PX_QUANT_SIZE=64)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
   <span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
   <span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Afterwards, inference will be accelerated on the DPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>To save and load the built module, one can use the typical TVM API’s:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lib_path</span> <span class="o">=</span> <span class="s2">&quot;deploy_lib.so&quot;</span>
<span class="n">lib</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>
</pre></div>
</div>
<p>Load the module from compiled files and run inference</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the module into memory</span>
<span class="n">loaded_lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
<span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="edge-usage">
<h3>Edge usage<a class="headerlink" href="#edge-usage" title="永久链接至标题">¶</a></h3>
<p>This section shows how to accelerate a convolutional neural network
model in TVM with Vitis-AI at the edge. The first couple of steps will
have to be run on the host machine and take care of quantization and
compilation for deployment at the edge.</p>
<p>A complete ResNet 18 example can be found <a class="reference external" href="https://github.com/Xilinx/pyxir/tree/master/examples/tvm">here</a>.</p>
<div class="section" id="host-steps">
<h4>Host steps<a class="headerlink" href="#host-steps" title="永久链接至标题">¶</a></h4>
<p>To be able to target the Vitis-AI cloud DPUCZDX8G we first have
to import the DPU target in PyXIR. This PyXIR package is the interface being
used by TVM to integrate with the Vitis-AI stack. Additionaly, import
the typical TVM and Relay modules and the Vitis-AI contrib module inside
TVM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyxir</span>
<span class="kn">import</span> <span class="nn">pyxir.contrib.target.DPUCZDX8G</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.relay</span> <span class="kn">as</span> <span class="nn">relay</span>
<span class="kn">from</span> <span class="nn">tvm.contrib.target</span> <span class="kn">import</span> <span class="n">vitis_ai</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">graph_executor</span>
<span class="kn">from</span> <span class="nn">tvm.relay.build_module</span> <span class="kn">import</span> <span class="n">bind_params_by_name</span>
<span class="kn">from</span> <span class="nn">tvm.relay.op.contrib.vitis_ai</span> <span class="kn">import</span> <span class="n">partition_for_vitis_ai</span>
</pre></div>
</div>
<p>After importing a convolutional neural network model using the usual
Relay API’s, annotate the Relay expression for the given Vitis-AI DPU
and partition the graph.</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>We recommend converting DPU convolutions’ data layouts to NHWC and CPU convolutions’
data layouts to NCHW for best DPU and out of the box CPU performance. You can use the
ConvertLayout transformation pass two times to achieve this as demonstrated in the code
block underneath. You can also leave the CPU convolution layouts in NHWC and tune ARM CPU
performance for this data layout to avoid the layout transformation overheads introduced by
executing DPU convolutions in NHWC and CPU convolutions in NCHW
(check out the <a class="reference external" href="https://tvm.apache.org/docs/tutorials/index.html#autoscheduler-template-free-auto-scheduling">AutoScheduling</a>
and <a class="reference external" href="https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_arm.html">AutoTuning</a>
tutorials for this).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bind_params_by_name</span><span class="p">(</span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">)</span>

<span class="c1"># For edge DPU we recommend converting the convolutions&#39; data layout</span>
<span class="c1">#    to NHWC for best performance. Therefore, we first convert the layouts</span>
<span class="c1">#    of all convolutions to NHWC before partitioning. Afterwards, we can</span>
<span class="c1">#    convert any remaining convolutions (to be executed on CPU) back to NCHW.</span>
<span class="n">desired_layouts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;nn.conv2d&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;default&#39;</span><span class="p">]}</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RemoveUnusedFunctions</span><span class="p">(),</span>
                                <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">ConvertLayout</span><span class="p">(</span><span class="n">desired_layouts</span><span class="p">),</span>
                                <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FoldConstant</span><span class="p">()])</span>
<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">seq</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

<span class="n">dpu</span> <span class="o">=</span> <span class="s1">&#39;DPUCZDX8G-zcu104&#39;</span>
<span class="c1"># Annotate and partition the Relay expression for the given DPU</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">partition_for_vitis_ai</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">dpu</span><span class="p">)</span>

<span class="c1"># After partitioning we recommend transforming the remaining convolutions</span>
<span class="c1">#    (that will be executed on CPU, if any) back to NCHW data layout</span>
<span class="c1">#    for best CPU performance</span>
<span class="n">desired_layouts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;nn.conv2d&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;default&#39;</span><span class="p">]}</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RemoveUnusedFunctions</span><span class="p">(),</span>
                                <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">ConvertLayout</span><span class="p">(</span><span class="n">desired_layouts</span><span class="p">),</span>
                                <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FoldConstant</span><span class="p">()])</span>
<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">seq</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can build the TVM runtime library for executing the model. The
TVM target is ‘llvm’ as the operations that can’t be handled by the DPU
are executed on the CPU. At this point that means the CPU on the host machine.
The Vitis-AI DPU identifier is DPUCZDX8G-zcu104 as we are targeting the edge DPU
on the ZCU104 board and this identifier is passed as a config to the TVM
build call. Note that different identifiers can be passed for different
DPU’s, see <a class="reference external" href="#edge-requirements">edge DPU’s info</a>. Additionally, we
provide the ‘export_runtime_module’ config that points to a file to which we
can export the Vitis-AI runtime module. We have to do this because we will
first be compiling and quantizing the model on the host machine before building
the model for edge deployment. As you will see later on, the exported runtime
module will be passed to the edge build so that the Vitis-AI runtime module
can be included.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="s1">&#39;llvm&#39;</span>
<span class="n">export_rt_mod_file</span> <span class="o">=</span> <span class="s2">&quot;vitis_ai.rtmod&quot;</span>

<span class="n">build_options</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;dpu&#39;</span><span class="p">:</span> <span class="n">dpu</span><span class="p">,</span>
   <span class="s1">&#39;export_runtime_module&#39;</span><span class="p">:</span> <span class="n">export_rt_mod_file</span>
<span class="p">}</span>
<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options&#39;</span><span class="p">:</span> <span class="n">build_options</span><span class="p">}):</span>
   <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>We will quantize and compile the model for execution on the DPU using on-the-fly
quantization on the host machine. This makes use of TVM inference calls
(module.run) to quantize the model on the host with the first N inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>

<span class="c1"># First N (default = 128) inputs are used for quantization calibration and will</span>
<span class="c1"># be executed on the CPU</span>
<span class="c1"># This config can be changed by setting the &#39;PX_QUANT_SIZE&#39; (e.g. export PX_QUANT_SIZE=64)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
   <span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
   <span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Save the TVM lib module so that the Vitis-AI runtime module will also be exported
(to the ‘export_runtime_module’ path we previously passed as a config).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>
<span class="n">lib</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;tvm_lib.so&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>After quantizing and compiling the model for Vitis-AI acceleration using the
first N inputs we can build the model for execution on the ARM edge device.
Here we pass the previously exported Vitis-AI runtime module so it can be included
in the TVM build.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Export lib for aarch64 target</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">arm_cpu</span><span class="p">(</span><span class="s1">&#39;ultra96&#39;</span><span class="p">)</span>
<span class="n">lib_kwargs</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;fcompile&#39;</span><span class="p">:</span> <span class="n">contrib</span><span class="o">.</span><span class="n">cc</span><span class="o">.</span><span class="n">create_shared</span><span class="p">,</span>
     <span class="s1">&#39;cc&#39;</span><span class="p">:</span> <span class="s2">&quot;/usr/aarch64-linux-gnu/bin/ld&quot;</span>
<span class="p">}</span>

<span class="n">build_options</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;load_runtime_module&#39;</span><span class="p">:</span> <span class="n">export_rt_mod_file</span>
<span class="p">}</span>
<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options&#39;</span><span class="p">:</span> <span class="n">build_options</span><span class="p">}):</span>
     <span class="n">lib_arm</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

<span class="n">lib_dpuv2</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="s1">&#39;tvm_dpu_arm.so&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">lib_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, move the TVM build files (tvm_dpu_arm.json, tvm_dpu_arm.so,
tvm_dpu_arm.params) to the edge device. For information on setting
up the edge device check out the <a class="reference external" href="#edge-dpuczdx8g">edge setup</a>
section.</p>
</div>
<div class="section" id="edge-steps">
<h4>Edge steps<a class="headerlink" href="#edge-steps" title="永久链接至标题">¶</a></h4>
<p>After setting up TVM with Vitis-AI on the edge device, you can now load
the TVM runtime module into memory and feed inputs for inference. A nearly
complete runtiem script can be found underneath. Make sure to run the script
as root (execute <code class="docutils literal notranslate"><span class="pre">su</span></code> in terminal to log into root).</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>You will see a warning about the ‘cpu-tf’ runtime not being found. This warning is
expected on the board and can be ignored. Note also that you <strong>shouldn’t</strong> import the
PyXIR DPU targets in the run script (<code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pyxir.contrib.target.DPUCZDX8G</span></code>).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyxir</span>
<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">graph_executor</span>

<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="c1"># input_name = ...</span>
<span class="c1"># input_data = ...</span>

<span class="c1"># load the module into memory</span>
<span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="s2">&quot;tvm_dpu_arm.so&quot;</span><span class="p">)</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">dev</span><span class="p">))</span>
<span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bnns.html" class="btn btn-neutral float-right" title="Relay BNNS Integration" accesskey="n" rel="next">下一个 <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tensorrt.html" class="btn btn-neutral float-left" title="Relay TensorRT Integration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> 上一个</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static//img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <ul class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <li class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2020 Apache Software Foundation | All right reserved</h5>
        </li>
      </ul>

    </div>

    <ul>
      <li class="footernote">Copyright © 2020 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</li>
    </ul>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>